# -*- coding: utf-8 -*-
"""Delayed flights prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15sdMnCyhp5Wtc3iotAnjhRJSBmI7QjF_

# Set up
"""

!pip install category_encoders
import os, random, numpy as np, pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

from google.colab import drive
# monta il tuo Google Drive
drive.mount('/content/drive')

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)

"""# Data Loading

Load flight dataset and create binary target variable. Flights with `ARR_DELAY` ≥ 15 minutes are classified as delayed. Missing values in delay cause columns are expected (only populated when delays occur).


"""

RAW_PATH = "/content/drive/MyDrive/Colab/archive/flights_sample.csv"
YEAR      = 2022                 # keep 2022
OUT_PATH  = f"data/flights_{YEAR}sample.csv"


# Essential columns for the mini-project (removing ~60 useless ones)
cols = ["FL_DATE", "AIRLINE","ORIGIN", "DEST",
        "CRS_DEP_TIME", "DEP_DELAY",
        "CRS_ARR_TIME", "ARR_DELAY",
        "DISTANCE"]

# ---- Targeted reading with parse_dates ---------------
df = (pd.read_csv(
        RAW_PATH,
        usecols=cols,
        parse_dates=["FL_DATE"],
        dtype={
            "AIRLINE": "category",
            "ORIGIN": "category",
            "DEST": "category",
            "CRS_DEP_TIME": "int32",
            "DEP_DELAY": "float32",
            "CRS_ARR_TIME": "int32",
            "ARR_DELAY": "float32",
            "DISTANCE": "float32",
        })
        .query("FL_DATE.dt.year == @YEAR")
        .reset_index(drop=True)
)


# ---- Binary Target ------------------------------------------
df["Delayed"] = (df["ARR_DELAY"] >= 15).astype("int8")

print(f"Dataset loaded: {len(df):,} flights")
print(f"Period: {df['FL_DATE'].min()} - {df['FL_DATE'].max()}")
print(f"Delayed flights: {df['Delayed'].sum():,} ({df['Delayed'].mean()*100:.1f}%)")

# Basic statistics
print(f"\nBasic delay statistics:")
print(f"- Average arrival delay: {df['ARR_DELAY'].mean():.1f} min")
print(f"- Average departure delay: {df['DEP_DELAY'].mean():.1f} min")
print(f"- Average distance: {df['DISTANCE'].mean():.0f} miles")

missing = df.isna().mean().sort_values(ascending=False)
print("\nPercentage of missing values per column:")
for col, pct in missing.items():
    if pct > 0:
        print(f"  {col}: {pct*100:.1f}%")

# ---- Save the year-full CSV for subsequent steps ----------
os.makedirs(os.path.dirname(OUT_PATH), exist_ok=True)
df.to_csv(OUT_PATH, index=False)
print(f"\nSaved {OUT_PATH}")

"""# Exploratory Data Analysis (EDA)
Identify patterns and correlations in flight delay data. Key insights: strong correlation between departure and arrival delays, seasonal patterns, and airline-specific performance differences.
"""

df = pd.read_csv(f"data/flights_{YEAR}sample.csv",
                 parse_dates=["FL_DATE"])

# Delay proportion -------------------
print(df["Delayed"].value_counts(normalize=True).rename("share"))

# Main distributions
fig, axes = plt.subplots(2, 2, figsize=(18, 12))

# 1. Delays by airline (top 10)
top10_airlines = df['AIRLINE'].value_counts().head(10).index
airline_delays = df[df['AIRLINE'].isin(top10_airlines)].groupby('AIRLINE')['Delayed'].mean().sort_values(ascending=False)
airline_delays.plot(kind='bar', ax=axes[0,1])
axes[0,1].set_title('Delay Rate by Airline (Top 10)')
axes[0,1].tick_params(axis='x', rotation=45)

# 2. Delays by month
monthly_delays = df.groupby(df['FL_DATE'].dt.month)['Delayed'].mean()
monthly_delays.plot(kind='line', marker='o', ax=axes[0,0])
axes[0,0].set_title('Delay Rate by Month')
axes[0,0].set_xlabel('Month')

# 3. Delays by departure hour
df['DepHour'] = df['CRS_DEP_TIME'] // 100
hourly_delays = df.groupby('DepHour')['Delayed'].mean()
hourly_delays.plot(kind='line', marker='o', ax=axes[1,0])
axes[1,0].set_title('Delay Rate by Departure Hour')
axes[1,0].set_xlabel('Hour')


# 4. Correlation DEP_DELAY vs ARR_DELAY (sample)
sample_data = df.sample(n=5000, random_state=42)
axes[1,1].scatter(sample_data['DEP_DELAY'], sample_data['ARR_DELAY'], alpha=0.6, s=20)
axes[1,1].set_title('DEP_DELAY vs ARR_DELAY (sample)')
axes[1,1].set_xlabel('Departure Delay (min)')
axes[1,1].set_ylabel('Arrival Delay (min)')

plt.tight_layout()
plt.show()

print("\nFirst rows of the DataFrame:")
display(df.head())

"""# Feaure engineering
Create temporal features (hour, day, season) and historical statistics (airline/route delay rates). Implement temporal split to prevent data leakage - model trains on past data to predict future delays.
"""

def add_temporal_features(dataframe):
    """Adds temporal and derived features"""
    df_copy = dataframe.copy()

    # Basic temporal features
    df_copy['Month'] = df_copy['FL_DATE'].dt.month
    df_copy['DayOfWeek'] = df_copy['FL_DATE'].dt.dayofweek
    df_copy['DayOfMonth'] = df_copy['FL_DATE'].dt.day
    df_copy['Quarter'] = df_copy['FL_DATE'].dt.quarter

    # Hours
    df_copy['DepHour'] = (df_copy['CRS_DEP_TIME'] // 100).astype(int)
    df_copy['ArrHour'] = (df_copy['CRS_ARR_TIME'] // 100).astype(int)

    # Useful binary features
    df_copy['IsWeekend'] = (df_copy['DayOfWeek'] >= 5).astype(int)
    df_copy['IsRushHour'] = ((df_copy['DepHour'].between(6, 9)) |
                            (df_copy['DepHour'].between(17, 20))).astype(int)
    df_copy['IsEarlyMorning'] = (df_copy['DepHour'] < 6).astype(int)
    df_copy['IsLateNight'] = (df_copy['DepHour'] >= 22).astype(int)

    # Distance features
    df_copy['FlightDuration'] = df_copy['DISTANCE'] / 500  # estimated duration in hours
    df_copy['IsLongFlight'] = (df_copy['DISTANCE'] > 1500).astype(int)
    df_copy['IsShortFlight'] = (df_copy['DISTANCE'] < 500).astype(int)

    # Advanced seasonality
    df_copy['IsHolidaySeason'] = df_copy['Month'].isin([11, 12, 1]).astype(int)
    df_copy['IsSummerSeason'] = df_copy['Month'].isin([6, 7, 8]).astype(int)

    return df_copy

# Temporal split to avoid data leakage
cutoff_date = df['FL_DATE'].quantile(0.7)
train_df = df[df['FL_DATE'] <= cutoff_date].copy()
temp_df = df[df['FL_DATE'] > cutoff_date].copy()

from sklearn.model_selection import train_test_split
valid_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42,
                                    stratify=temp_df['Delayed'])

print(f"Split dataset:")
print(f"- Train: {len(train_df):,} ({len(train_df)/len(df)*100:.1f}%)")
print(f"- Valid: {len(valid_df):,} ({len(valid_df)/len(df)*100:.1f}%)")
print(f"- Test: {len(test_df):,} ({len(test_df)/len(df)*100:.1f}%)")

# Apply feature engineering
train_df = add_temporal_features(train_df)
valid_df = add_temporal_features(valid_df)
test_df = add_temporal_features(test_df)

# Historical statistics calculated only on training set
print("Creating historical statistics")

# Statistics by airline (including DEP_DELAY)
airline_stats = train_df.groupby('AIRLINE').agg({
    'Delayed': 'mean',
    'DEP_DELAY': 'mean'
}).round(3)
airline_stats.columns = ['delay_rate_airline', 'avg_dep_delay_airline']

# Statistics by route
route_stats = train_df.groupby(['ORIGIN', 'DEST']).agg({
    'Delayed': 'mean',
    'DISTANCE': 'mean'
}).round(3)
route_stats.columns = ['delay_rate_route', 'avg_distance_route']

# Statistics by origin (including DEP_DELAY)
origin_stats = train_df.groupby('ORIGIN').agg({
    'Delayed': 'mean',
    'DEP_DELAY': 'mean'
}).round(3)
origin_stats.columns = ['delay_rate_origin', 'avg_dep_delay_origin']

# Statistics by hour
hour_stats = train_df.groupby('DepHour')['Delayed'].mean().round(3)
hour_stats.name = 'delay_rate_hour'

# Merge the statistics
def merge_stats(df_set):
    df_merged = df_set.merge(airline_stats, on='AIRLINE', how='left')
    df_merged = df_merged.merge(route_stats, on=['ORIGIN', 'DEST'], how='left')
    df_merged = df_merged.merge(origin_stats, on='ORIGIN', how='left')
    df_merged = df_merged.merge(hour_stats, on='DepHour', how='left')
    return df_merged

train_df = merge_stats(train_df)
valid_df = merge_stats(valid_df)
test_df = merge_stats(test_df)

# Fill NaN with default values
fill_cols = ['delay_rate_airline', 'avg_dep_delay_airline', 'delay_rate_route',
             'avg_distance_route', 'delay_rate_origin', 'avg_dep_delay_origin', 'delay_rate_hour']

for col in fill_cols:
    train_df[col] = train_df[col].fillna(train_df[col].mean())
    valid_df[col] = valid_df[col].fillna(train_df[col].mean())
    test_df[col] = test_df[col].fillna(train_df[col].mean())

print(f"Total features created: {len(train_df.columns) - 1}")

"""# Data Preparation
Build preprocessing pipeline for post-departure prediction scenario. StandardScaler for numerical features, OneHotEncoder for low-cardinality categoricals, TargetEncoder for high-cardinality features.
"""

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
import category_encoders as ce

# IMPORTANT: To predict AFTER departure, we include the departure delay
# We use DEP_DELAY (actual departure delay that has already occurred)
print("TARGET: Predict arrival delays AFTER departure")
print("Features used: pre-flight info + actual departure delay")

# Definition of feature groups (WITH DEP_DELAY)
num_cols = ['DISTANCE', 'DEP_DELAY', 'DepHour', 'ArrHour', 'Month', 'DayOfWeek', 'DayOfMonth', 'Quarter',
            'IsWeekend', 'IsRushHour', 'IsEarlyMorning', 'IsLateNight', 'FlightDuration',
            'IsLongFlight', 'IsShortFlight', 'IsHolidaySeason', 'IsSummerSeason'] + fill_cols

cat_cols = ['AIRLINE', 'ORIGIN', 'DEST']

# Separation by cardinality
low_card_cat_cols = [col for col in cat_cols if train_df[col].nunique() <= 25]
high_card_cat_cols = [col for col in cat_cols if train_df[col].nunique() > 25]

print(f"Numerical features: {len(num_cols)}")
print(f"Low cardinality categorical features: {len(low_card_cat_cols)} - {low_card_cat_cols}")
print(f"High cardinality categorical features: {len(high_card_cat_cols)} - {high_card_cat_cols}")

# Preprocessing pipelines
numeric_tf = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

low_card_cat_tf = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

high_card_cat_tf = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('target_enc', ce.TargetEncoder(handle_unknown='value'))
])

preprocessor = ColumnTransformer([
    ('num', numeric_tf, num_cols),
    ('low_cat', low_card_cat_tf, low_card_cat_cols),
    ('high_cat', high_card_cat_tf, high_card_cat_cols)
])

"""# Random Forest Model
Train Random Forest with RandomizedSearchCV for hyperparameter optimization. Custom F-beta scoring emphasizes recall to minimize missed delays (business priority).
"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import TimeSeriesSplit, RandomizedSearchCV
from sklearn.metrics import make_scorer, precision_score, recall_score

# Data preparation
feature_cols = num_cols + cat_cols
X_train = train_df[feature_cols]
y_train = train_df['Delayed']
X_valid = valid_df[feature_cols]
y_valid = valid_df['Delayed']
X_test = test_df[feature_cols]
y_test = test_df['Delayed']

# Full pipeline
rf_pipeline = Pipeline([
    ('prep', preprocessor),
    ('model', RandomForestClassifier(random_state=42, n_jobs=-1))
])

# Hyperparameter search (optimized for speed)
param_grid = {
    'model__n_estimators': [100, 200, 300],
    'model__max_depth': [None, 20, 30],
    'model__min_samples_split': [2, 5],
    'model__min_samples_leaf': [1, 2],
    'model__class_weight': ['balanced', 'balanced_subsample']
}

# Custom scorer to balance precision and recall
def custom_f_beta(y_true, y_pred):
    precision = precision_score(y_true, y_pred, zero_division=0)
    recall = recall_score(y_true, y_pred, zero_division=0)
    if precision + recall == 0:
        return 0
    # F-beta with beta=1.5 (more weight to recall)
    return (1 + 1.5**2) * precision * recall / (1.5**2 * precision + recall)

# Temporal cross-validation
tscv = TimeSeriesSplit(n_splits=3)

# Sampling to speed up (use 100k samples for tuning)
sample_size = min(100000, len(X_train))
sample_idx = np.random.choice(len(X_train), sample_size, replace=False)
X_tune = X_train.iloc[sample_idx]
y_tune = y_train.iloc[sample_idx]

print(f"Hyperparameter tuning on {len(X_tune):,} samples...")
grid_search = RandomizedSearchCV(
    estimator=rf_pipeline,
    param_distributions=param_grid,
    n_iter=15,  # Number of combinations to test
    cv=tscv,
    scoring=make_scorer(custom_f_beta),
    n_jobs=-1,
    verbose=1,
    random_state=42
)

grid_search.fit(X_tune, y_tune)

print(f"Best parameters: {grid_search.best_params_}")
print(f"Best CV score: {grid_search.best_score_:.3f}")


print("FINAL TRAINING...")

# Training on train + validation
best_model = grid_search.best_estimator_
X_train_full = pd.concat([X_train, X_valid])
y_train_full = pd.concat([y_train, y_valid])

best_model.fit(X_train_full, y_train_full)

"""# Evaluation on Test
Evaluate final model performance on unseen test data. Calculate precision, recall, ROC-AUC and analyze feature importance to validate modeling approach.
"""

from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
from sklearn.metrics import precision_recall_curve, RocCurveDisplay, PrecisionRecallDisplay

# Predictions
y_pred = best_model.predict(X_test)
y_prob = best_model.predict_proba(X_test)[:, 1]

# Basic metrics
print("=== TEST SET RESULTS ===")
print(classification_report(y_test, y_pred, digits=3))
print(f"ROC-AUC: {roc_auc_score(y_test, y_prob):.3f}")


fig, axes = plt.subplots(2, 2, figsize=(15, 12))

# 1. Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,0])
axes[0,0].set_title('Confusion Matrix')
axes[0,0].set_xlabel('Predicted')
axes[0,0].set_ylabel('Actual')

# 2. ROC Curve
RocCurveDisplay.from_predictions(y_test, y_prob, ax=axes[0,1])
axes[0,1].set_title('ROC Curve')
axes[0,1].grid(True)

# 3. Precision-Recall Curve
PrecisionRecallDisplay.from_predictions(y_test, y_prob, ax=axes[1,0])
axes[1,0].set_title('Precision-Recall Curve')
axes[1,0].legend()
axes[1,0].grid(True)


# 4. Feature Importances
importances = best_model.named_steps['model'].feature_importances_
feature_names = best_model.named_steps['prep'].get_feature_names_out()

imp_df = pd.DataFrame({
    'feature': feature_names,
    'importance': importances
}).sort_values('importance', ascending=False).head(15)

sns.barplot(data=imp_df, x='importance', y='feature', ax=axes[1,1])
axes[1,1].set_title('Top 15 Feature Importances')

plt.tight_layout()
plt.show()

"""## Model Comparison Benchmark
Compare Random Forest against classical ML algorithms (Decision Tree, Logistic Regression, SVM) to validate model selection and demonstrate superior performance.
"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, roc_auc_score, RocCurveDisplay, f1_score
import matplotlib.pyplot as plt

# Dictionary of models to compare
models_to_compare = {
    'Random Forest': grid_search.best_estimator_
    ,
    'Decision Tree': DecisionTreeClassifier(
        max_depth=15,
        class_weight='balanced',
        random_state=RANDOM_SEED
    ),
    'Logistic Regression': LogisticRegression(
        class_weight='balanced',
        random_state=RANDOM_SEED,
        max_iter=1000
    ),
    'K-Nearest Neighbors': KNeighborsClassifier(
        n_neighbors=5,
        weights='distance'
    ),
    'Naive Bayes': GaussianNB()
}

# Training and evaluation of all models
results = {}
models_trained = {}

print("Training models in progress...")
for name, model in models_to_compare.items():
    print(f"  - {name}...")

    # Create pipeline
    if name == 'Random Forest':
        pipeline = model
    else:
        pipeline = Pipeline([
          ('prep', preprocessor),
          ('model', model)
        ])

    # Training
    if name != 'Random Forest':
      pipeline.fit(X_train_full, y_train_full)

    # Predictions
    y_pred_model = pipeline.predict(X_test)
    y_prob_model = pipeline.predict_proba(X_test)[:, 1]

    # Save results
    results[name] = {
        'predictions': y_pred_model,
        'probabilities': y_prob_model,
        'roc_auc': roc_auc_score(y_test, y_prob_model),
        'precision': precision_score(y_test, y_pred_model, zero_division=0),
        'recall': recall_score(y_test, y_pred_model, zero_division=0),
        'f1': f1_score(y_test, y_pred_model, zero_division=0)
    }
    models_trained[name] = pipeline

# Print comparative results
print("\n=== COMPARISON RESULTS ===")
print(f"{'Model':<20} {'ROC-AUC':<8} {'Precision':<10} {'Recall':<8} {'F1-Score':<8}")
print("-" * 60)

for name, metrics in sorted(results.items(), key=lambda x: x[1]['roc_auc'], reverse=True):
    print(f"{name:<20} {metrics['roc_auc']:.3f}    {metrics['precision']:.3f}      "
          f"{metrics['recall']:.3f}    {metrics['f1']:.3f}")

# Visual comparison of ROC curves
fig, axes = plt.subplots(1, 2, figsize=(15, 6))

# ROC Curves
ax1 = axes[0]
for name, metrics in results.items():
    RocCurveDisplay.from_predictions(y_test, metrics['probabilities'],
                                   name=name, ax=ax1)
ax1.set_title('ROC Curve Comparison')
ax1.grid(True)
ax1.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

# Bar plot of metrics
ax2 = axes[1]
model_names = list(results.keys())
roc_aucs = [results[name]['roc_auc'] for name in model_names]
recalls = [results[name]['recall'] for name in model_names]

x = np.arange(len(model_names))
width = 0.35

bars1 = ax2.bar(x - width/2, roc_aucs, width, label='ROC-AUC', alpha=0.8)
bars2 = ax2.bar(x + width/2, recalls, width, label='Recall', alpha=0.8)

ax2.set_xlabel('Models') # Translated comment
ax2.set_ylabel('Score')
ax2.set_title('ROC-AUC vs Recall Comparison') # Translated comment
ax2.set_xticks(x)
ax2.set_xticklabels(model_names, rotation=45, ha='right')
ax2.legend()
ax2.grid(True, alpha=0.3)

# Add values on bars
for bar in bars1:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}', ha='center', va='bottom', fontsize=8)

for bar in bars2:
    height = bar.get_height()
    ax2.text(bar.get_x() + bar.get_width()/2., height + 0.005,
             f'{height:.3f}', ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()

# Identify the best for each metric
best_roc_auc = max(results.items(), key=lambda x: x[1]['roc_auc'])
best_recall = max(results.items(), key=lambda x: x[1]['recall'])
best_f1 = max(results.items(), key=lambda x: x[1]['f1'])

print(f"\n🏆 BEST MODELS PER METRIC:") # Translated comment
print(f"  ROC-AUC: {best_roc_auc[0]} ({best_roc_auc[1]['roc_auc']:.3f})")
print(f"  Recall:  {best_recall[0]} ({best_recall[1]['recall']:.3f})")
print(f"  F1-Score: {best_f1[0]} ({best_f1[1]['f1']:.3f})")

# Save the best models for later use
best_model_overall = models_trained[best_f1[0]]  # Use F1 as general metric
y_prob = results[best_f1[0]]['probabilities']  # Update for subsequent sections